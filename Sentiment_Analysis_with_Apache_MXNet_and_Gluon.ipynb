{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Apache MXNet and Gluon\n",
    "\n",
    "This tutorial walks you through how to implement a sentiment analysis model to classify movie reviews as either 'Positive' or 'Negative' using Apache MXNet and the Gluon programming interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, autograd\n",
    "from mxnet.gluon import nn, rnn\n",
    "\n",
    "context = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to load the movie review dataset. We will be taking advantage of Stanford's Large Movie Review Dataset that is available here: http://ai.stanford.edu/~amaas/data/sentiment/. This dataset includes 25,000 movies reviews from the IMBD database with 12,500 labeled as 'Positive' reviews and the other 12,500 labeled as 'Negative' reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(foldername):\n",
    "    import os\n",
    "    sentiments = []\n",
    "    filenames = os.listdir(os.curdir+ \"/\"+foldername)\n",
    "    for file in filenames:\n",
    "        with open(foldername+\"/\"+file,\"r\", encoding=\"utf8\") as pos_file:\n",
    "            data=pos_file.read().replace('\\n', '')\n",
    "            sentiments.append(data)\n",
    "    return sentiments\n",
    "    \n",
    "    \n",
    "#Ensure that the path below leads to the location of the positive reviews \n",
    "foldername = \"aclImdb/train/pos/\"\n",
    "postive_sentiment = read_files(foldername)\n",
    "\n",
    "#Ensure that the path below leads to the location of the negative reviews\n",
    "foldername = \"aclImdb/train/neg/\"\n",
    "negative_sentiment = read_files(foldername)\n",
    "\n",
    "#This labels the 'Positive' reviews as 1' and the 'Negative' reviews as 0\n",
    "positive_labels = [1 for _ in postive_sentiment]\n",
    "negative_labels = [0 for _ in negative_sentiment]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to clean up the text of the movie reviews so that we are only processing words. The actual words in the reviews are going to be the most predictive - not sentence breaks or commas, for example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some string preprocessing\n",
    "def clean_str(string):  \n",
    "    \n",
    "    #This removes any special characters from the review\n",
    "    remove_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "    \n",
    "    #This removes any line breaks and replaces them with spaces\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    \n",
    "    return re.sub(remove_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to process all of the words in the reviews, count the number of occurences of each word, and then index the words in descending order with respect to how many times this occur. This is a necessary input to help us encode the words in the reviews so that they can be understood by a machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates a dictionary of the words and their counts in entire \n",
    "#movie review dataset {word:count}\n",
    "\n",
    "word_counter = Counter()\n",
    "def create_count(sentiments):\n",
    "    for line in sentiments:\n",
    "        for word in (clean_str(line)).split():\n",
    "            if word not in word_counter.keys():               \n",
    "                word_counter[word] = 1\n",
    "            else:\n",
    "                word_counter[word] += 1\n",
    "\n",
    "#This assigns a unique a number for each word (sorted by descending order \n",
    "#based on the frequency of occurrence)and returns a word_dict\n",
    "\n",
    "def create_word_index():\n",
    "    idx = 1\n",
    "    word_dict = {}\n",
    "    for word in word_counter.most_common():\n",
    "        word_dict[word[0]] = idx\n",
    "        idx+=1\n",
    "    return word_dict\n",
    "    \n",
    "#Here we combine all of the reviews into one dataset and create a word\n",
    "#dictionary using this entire dataset\n",
    "\n",
    "all_sentiments = postive_sentiment + negative_sentiment\n",
    "all_labels = positive_labels + negative_labels\n",
    "create_count(all_sentiments)\n",
    "word_dict = create_word_index()\n",
    "\n",
    "#This creates a reverse index from a number to the word \n",
    "idx2word = {v: k for k, v in word_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a set of helper functions that (1) encode words into a sequence of numbers, (2) decode a sequence of numbers back into words, and (3) truncate and pad the input data to ensure they are of equal length and thereby enable easier processing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This helper function creates a encoded sentences by assigning the unique \n",
    "#id from word_dict to the words in the input text (i.e., movie reviews)\n",
    "def encoded_sentences(input_file,word_dict):\n",
    "    output_string = []\n",
    "    for line in input_file:\n",
    "        output_line = []\n",
    "        for word in (clean_str(line)).split():\n",
    "            if word in word_dict:\n",
    "                output_line.append(word_dict[word])\n",
    "        output_string.append(output_line)\n",
    "    return output_string\n",
    "\n",
    "#This helper function decodes encoded sentences\n",
    "def decode_sentences(input_file,word_dict):\n",
    "    output_string = []\n",
    "    for line in input_file:\n",
    "        output_line = ''\n",
    "        for idx in line:\n",
    "            output_line += idx2word[idx] + ' '\n",
    "        output_string.append(output_line)\n",
    "    return output_string\n",
    "\n",
    "#This helper function pads the sequences to maxlen.\n",
    "#If the sentence is greater than maxlen, it truncates the sentence.\n",
    "#If the sentence is less than 500, it pads with value 0.\n",
    "def pad_sequences(sentences,maxlen=500,value=0):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by maxlen.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    padded_sentences = []\n",
    "    for sen in sentences:\n",
    "        new_sentence = []\n",
    "        if(len(sen) > maxlen):\n",
    "            new_sentence = sen[:maxlen]\n",
    "            padded_sentences.append(new_sentence)\n",
    "        else:\n",
    "            num_padding = maxlen - len(sen)\n",
    "            new_sentence = np.append(sen,[value] * num_padding)\n",
    "            padded_sentences.append(new_sentence)\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to encode all of the movie reviews using the word dictionary created. In addition, we are going to cap the size of the tracked vocabulary size - meaning any word that is outside of the tracked range will be encoded with the last position. This is performance versus accuracy consideration - a larger tracked vocabulary will lead to more accurary but will have performance considerations because it requires a longer training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encodes the positive and negative reviews into sequences of number\n",
    "positive_encoded = encoded_sentences(postive_sentiment,word_dict)\n",
    "negative_encoded = encoded_sentences(negative_sentiment,word_dict)\n",
    "\n",
    "all_encoded = positive_encoded + negative_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000 #Here we set the total num of words to be tracked\n",
    "\n",
    "#Any word outside of the tracked range will be encoded with last position.\n",
    "t_data = [np.array([i if i<(vocab_size-1) else (vocab_size-1) for i in s]) for s in all_encoded]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a word embedding matrix to represent the words that we observe in the movie reviews. Represeting the meaning of the words with these vectors is a large exercise unto itself. Instead, we will be leveraging Stanford's Global Vector for Word Representation (GloVe) embedding. We specifically used glove.42B.300d.zip available at this link:\n",
    "https://nlp.stanford.edu/projects/glove/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads Stanford's Global Vector for Word Representation (GloVe) embedding\n",
    "\n",
    "num_embed = 300 #This is the richness of the word attributes captured\n",
    "\n",
    "def load_glove_index(loc):\n",
    "    f = open(loc, encoding=\"utf8\")\n",
    "    embeddings_index = {}\n",
    "    \n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index\n",
    "\n",
    "def create_emb():\n",
    "    embedding_matrix = np.zeros((vocab_size, num_embed))\n",
    "    for word, i in word_dict.items():\n",
    "        if i >= vocab_size:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    embedding_matrix = nd.array(embedding_matrix)\n",
    "    return embedding_matrix\n",
    "\n",
    "embeddings_index = load_glove_index('glove.42B.300d.txt')\n",
    "embedding_matrix = create_emb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we prepare the movie reviews to be fed into the deep learning model by (1) Reserving 30% of the dataset as a test dataset, (2) padding and truncating the data to the length of 500 words, and (3) converting the movie reviews into MXNet's NDArray format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This separates 30% of the entire dataset into test dataset.\n",
    "X_train, X_test, y_train, y_test_set = train_test_split(t_data, all_labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the minimum length is: 10\n",
      "the maximum length is: 2459\n",
      "the average length is: 230.51952\n"
     ]
    }
   ],
   "source": [
    "#Here are some of the statistics of sentences before padding\n",
    "min_len = min(map(len, t_data))\n",
    "max_len = max(map(len,t_data))\n",
    "avg_len = sum(map(len,t_data)) / len(t_data)\n",
    "print(\"the minimum length is:\",min_len)\n",
    "print(\"the maximum length is:\",max_len)\n",
    "print(\"the average length is:\",avg_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 500 #This set the max word length of each movie review\n",
    "\n",
    "#Below we pad the reviews and convert them to MXNet's NDArray format\n",
    "trn = nd.array(pad_sequences(X_train, maxlen=seq_len, value=0))\n",
    "test = nd.array(pad_sequences(X_test, maxlen=seq_len, value=0))\n",
    "y_trn = nd.array(y_train)\n",
    "y_test = nd.array(y_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to define the neural network for this model using Gluon. We will be using an LSTM model with 64 hidden units, and we will be taking advantage of the embedding layer created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "num_hidden = 64\n",
    "learning_rate = .001\n",
    "epochs = 10\n",
    "batch_size = 12\n",
    "\n",
    "model = mx.gluon.nn.Sequential()\n",
    "\n",
    "with model.name_scope():    \n",
    "    model.embed = mx.gluon.nn.Embedding(vocab_size, num_embed)\n",
    "    model.add(mx.gluon.rnn.LSTM(num_hidden, layout = 'NTC'))\n",
    "    model.add(mx.gluon.nn.Dense(num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we execute the training loop, we need to define a function that will calculate the accurary metrics for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model,x,y,batch_size):\n",
    "    \n",
    "    acc = mx.metric.Accuracy()\n",
    "    \n",
    "    for i in range(x.shape[0] // batch_size):\n",
    "        data = x[i*batch_size:(i*batch_size + batch_size),]\n",
    "        target = y[i*batch_size:(i*batch_size + batch_size),]\n",
    "    \n",
    "        data = data.as_in_context(context)\n",
    "        target = target.as_in_context(context)\n",
    "        \n",
    "        output = model(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=target)\n",
    "    \n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to execute the training loop. Prior to kicking off the training loop, we need to initialize the model parameters and the optimer function in addition to setting up the pre-training embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ping\n",
      "Epoch 0. Train_acc 0.622133333333, Test_acc 0.635802469136\n",
      "ping\n",
      "Epoch 1. Train_acc 0.664266666667, Test_acc 0.68032693187\n",
      "ping\n",
      "Epoch 2. Train_acc 0.726, Test_acc 0.738225880201\n",
      "ping\n",
      "Epoch 3. Train_acc 0.770133333333, Test_acc 0.779721079104\n",
      "ping\n",
      "Epoch 4. Train_acc 0.7864, Test_acc 0.793267032465\n",
      "ping\n",
      "Epoch 5. Train_acc 0.798266666667, Test_acc 0.800925925926\n",
      "ping\n",
      "Epoch 6. Train_acc 0.802533333333, Test_acc 0.805898491084\n",
      "ping\n",
      "Epoch 7. Train_acc 0.805466666667, Test_acc 0.812299954275\n",
      "ping\n",
      "Epoch 8. Train_acc 0.809466666667, Test_acc 0.816415180613\n",
      "ping\n",
      "Epoch 9. Train_acc 0.813333333333, Test_acc 0.820816186557\n"
     ]
    }
   ],
   "source": [
    "model.collect_params().initialize(mx.init.Xavier(), ctx=context)\n",
    "\n",
    "model.embed.weight.set_data(embedding_matrix.as_in_context(context))\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd',\n",
    "                        {'learning_rate': learning_rate})\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()    \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    print('ping')\n",
    "    for b in range(trn.shape[0] // batch_size):\n",
    "        data = trn[b*batch_size:(b*batch_size + batch_size),]\n",
    "        target = y_trn[b*batch_size:(b*batch_size + batch_size),]\n",
    "        \n",
    "        data = data.as_in_context(context)\n",
    "        target = target.as_in_context(context)\n",
    "        \n",
    "        with autograd.record():\n",
    "            output = model(data)\n",
    "            L = softmax_cross_entropy(output, target)\n",
    "            L.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "            \n",
    "    test_accuracy = evaluate_accuracy(model, trn, y_trn, batch_size)\n",
    "    train_accuracy = evaluate_accuracy(model, test, y_test, batch_size)\n",
    "    print(\"Epoch %s. Train_acc %s, Test_acc %s\" %\n",
    "          (epoch, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mxnet_p36)",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
